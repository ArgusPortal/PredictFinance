# üìò Guia de Execu√ß√£o - Fase 7: Deploy da API no Render

## üìã √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Pr√©-requisitos](#pr√©-requisitos)
3. [Objetivos da Fase](#objetivos-da-fase)
4. [Prepara√ß√£o dos Arquivos](#prepara√ß√£o-dos-arquivos)
5. [Deploy Passo a Passo](#deploy-passo-a-passo)
6. [Testes em Produ√ß√£o](#testes-em-produ√ß√£o)
7. [Monitoramento e Logs](#monitoramento-e-logs)
8. [Troubleshooting](#troubleshooting)
9. [Checklist de Conclus√£o](#checklist-de-conclus√£o)
10. [Refer√™ncias](#refer√™ncias)

---

## üéØ Vis√£o Geral

A **Fase 7** realiza o deploy da API FastAPI no **Render.com**, tornando-a acess√≠vel publicamente via HTTPS. Esta fase transforma o servi√ßo local em uma API de produ√ß√£o na nuvem.

**Dura√ß√£o Estimada**: 30-60 minutos  
**Complexidade**: Intermedi√°ria  
**Plataforma**: Render.com (Free Tier)

---

## ‚úÖ Pr√©-requisitos

### Fases Anteriores

- ‚úÖ Fase 1-5: Modelo treinado e validado
- ‚úÖ Fase 6: API FastAPI funcionando localmente

### Contas Necess√°rias

- ‚úÖ **GitHub**: Reposit√≥rio com c√≥digo
- ‚úÖ **Render.com**: Conta gratuita (criar em https://render.com)

### Artefatos Necess√°rios

```
PredictFinance/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ lstm_model_best.h5  (0.39 MB)
‚îÇ   ‚îî‚îÄ‚îÄ scaler.pkl           (0.86 KB)
‚îú‚îÄ‚îÄ requirements-render.txt
‚îú‚îÄ‚îÄ render.yaml
‚îî‚îÄ‚îÄ Procfile
```

---

## üéØ Objetivos da Fase

1. ‚úÖ Preparar depend√™ncias otimizadas para produ√ß√£o
2. ‚úÖ Configurar arquivos de deploy (render.yaml, Procfile)
3. ‚úÖ Incluir modelos no reposit√≥rio Git
4. ‚úÖ Fazer deploy no Render.com
5. ‚úÖ Obter URL p√∫blica da API
6. ‚úÖ Testar todos os endpoints em produ√ß√£o
7. ‚úÖ Validar funcionalidade completa

---

## üì¶ Prepara√ß√£o dos Arquivos

### 1. Requirements Otimizado

**Arquivo**: `requirements-render.txt`

```txt
# Core Framework
fastapi==0.109.2
uvicorn[standard]==0.27.1
pydantic==2.5.3

# Machine Learning (CPU only)
tensorflow-cpu==2.15.1
scikit-learn==1.3.2
numpy==1.24.4

# Data Processing
pandas==2.0.3

# Model Persistence
joblib==1.5.2
```

**Por que tensorflow-cpu?**
- Reduz tamanho do build de ~2GB para ~500MB
- Free tier do Render tem limite de recursos
- Suficiente para infer√™ncia (n√£o precisa GPU)

### 2. Configura√ß√£o Render

**Arquivo**: `render.yaml`

```yaml
services:
  - type: web
    name: b3sa3-api
    env: python
    region: oregon
    plan: free
    buildCommand: pip install -r requirements-render.txt
    startCommand: uvicorn api.main:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: PYTHON_VERSION
        value: 3.10.11
```

### 3. Procfile (Backup)

**Arquivo**: `Procfile`

```
web: uvicorn api.main:app --host 0.0.0.0 --port $PORT
```

### 4. Atualizar .gitignore

O `.gitignore` foi atualizado para **incluir** os modelos no Git:

```gitignore
# Modelos (comentado para permitir deploy)
# models/*.h5
# models/*.pkl

# Por enquanto, os modelos s√£o versionados para o deploy
```

---

## üöÄ Deploy Passo a Passo

### Passo 1: Commit e Push para GitHub

```bash
# 1. Verificar arquivos modificados
git status

# 2. Adicionar arquivos de configura√ß√£o
git add requirements-render.txt
git add render.yaml
git add Procfile
git add .gitignore

# 3. Adicionar c√≥digo da API
git add api/
git add run_api.py

# 4. Adicionar modelos (IMPORTANTE!)
git add models/lstm_model_best.h5
git add models/scaler.pkl
git add models/model_architecture.json

# 5. Adicionar documenta√ß√£o
git add docs/DEPLOY_RENDER.md
git add docs/FASE_7_GUIA.md
git add DEPLOY_QUICKSTART.md
git add test_production.py

# 6. Commit
git commit -m "feat: Deploy no Render.com (Fase 7)

- Adicionar requirements-render.txt otimizado
- Configurar render.yaml para deploy autom√°tico
- Incluir modelos no reposit√≥rio para deploy
- Criar script de teste de produ√ß√£o
- Documenta√ß√£o completa de deploy"

# 7. Push
git push origin main
```

**Verificar Push**:
```bash
# Acessar GitHub e verificar se arquivos foram enviados
# https://github.com/ArgusPortal/PredictFinance
```

### Passo 2: Criar Conta no Render

1. Acesse: https://render.com/
2. Clique em **"Get Started for Free"**
3. Selecione **"Sign up with GitHub"**
4. Autorize o Render a acessar sua conta GitHub

### Passo 3: Criar Web Service

1. No Dashboard do Render, clique em **"New +"**
2. Selecione **"Web Service"**
3. Na lista de reposit√≥rios:
   - Procure por **"PredictFinance"**
   - Clique em **"Connect"**
   
**Se o reposit√≥rio n√£o aparecer**:
- Clique em **"Configure account"**
- Autorize acesso ao reposit√≥rio espec√≠fico

### Passo 4: Configurar o Service

Preencha os campos conforme abaixo:

#### Settings B√°sicos

| Campo | Valor |
|-------|-------|
| **Name** | `b3sa3-api` (ou nome de sua prefer√™ncia) |
| **Region** | `Oregon (US West)` |
| **Branch** | `main` |
| **Root Directory** | (deixar em branco) |

#### Build & Deploy

| Campo | Valor |
|-------|-------|
| **Runtime** | `Python 3` |
| **Build Command** | `pip install -r requirements-render.txt` |
| **Start Command** | `uvicorn api.main:app --host 0.0.0.0 --port $PORT` |

#### Instance Type

| Campo | Valor |
|-------|-------|
| **Plan** | **Free** |

### Passo 5: Environment Variables (Opcional)

N√£o s√£o necess√°rias vari√°veis de ambiente obrigat√≥rias, mas voc√™ pode adicionar:

| Key | Value | Descri√ß√£o |
|-----|-------|-----------|
| `PYTHON_VERSION` | `3.10.11` | Vers√£o do Python (j√° definido no render.yaml) |

### Passo 6: Criar e Deploy

1. Clique em **"Create Web Service"**
2. O Render iniciar√° o build automaticamente
3. Acompanhe o progresso na aba **"Logs"**

---

## üìä Monitoramento do Build

### Logs Esperados

Durante o build, voc√™ ver√° (pode levar ~5 minutos):

```
==> Cloning from https://github.com/ArgusPortal/PredictFinance...
==> Checked out commit abc123

==> Installing dependencies
==> Running 'pip install -r requirements-render.txt'
    Collecting fastapi==0.109.2
    Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)
    Collecting uvicorn[standard]==0.27.1
    Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)
    Collecting tensorflow-cpu==2.15.1
    Downloading tensorflow_cpu-2.15.1-cp310-cp310-manylinux2014_x86_64.whl (211.7 MB)
    ...
    Successfully installed fastapi-0.109.2 uvicorn-0.27.1 tensorflow-cpu-2.15.1 ...

==> Build successful ‚úì

==> Starting service
==> Running 'uvicorn api.main:app --host 0.0.0.0 --port $PORT'

üöÄ Iniciando API...
üìÇ Carregando artefatos do modelo...
   ‚îî‚îÄ Carregando modelo: /opt/render/project/src/models/lstm_model_best.h5
   ‚úÖ Modelo carregado com sucesso!
   ‚îî‚îÄ Carregando scaler: /opt/render/project/src/models/scaler.pkl
   ‚úÖ Scaler carregado com sucesso!
‚úÖ API pronta para receber requisi√ß√µes!

INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:10000

==> Your service is live üéâ
```

### Tempo de Build

- **Instalar depend√™ncias**: 3-5 minutos
- **Iniciar service**: 10-15 segundos
- **Total**: ~5 minutos

### Obter URL da API

Ap√≥s deploy bem-sucedido, a URL aparecer√° no topo:

```
https://b3sa3-api.onrender.com
```

**Copie esta URL** - voc√™ usar√° nos testes!

---

## üß™ Testes em Produ√ß√£o

### Teste 1: Health Check (cURL)

```bash
curl https://b3sa3-api.onrender.com/
```

**Resposta esperada**:
```json
{
  "status": "ativo",
  "mensagem": "API de previs√£o B3SA3.SA operacional",
  "versao": "1.0.0",
  "modelo_carregado": true
}
```

### Teste 2: Informa√ß√µes do Modelo

```bash
curl https://b3sa3-api.onrender.com/info
```

**Resposta esperada**:
```json
{
  "nome": "LSTM_B3SA3_Predictor",
  "arquitetura": "LSTM - 2 camadas (64 ‚Üí 32 unidades) + Dropout (0.2)",
  "parametros": 30369,
  "metricas": {
    "RMSE": "R$ 0.26",
    "MAE": "R$ 0.20",
    "MAPE": "1.53%",
    "R2": "0.9351"
  },
  "window_size": 60,
  "features": ["Open", "High", "Low", "Close", "Volume"]
}
```

### Teste 3: Script Automatizado

```bash
# Substituir pela URL real do Render
python test_production.py https://b3sa3-api.onrender.com
```

**Sa√≠da esperada**:

```
================================================================================
                        üß™ TESTE DA API EM PRODU√á√ÉO
================================================================================

üìç URL da API: https://b3sa3-api.onrender.com
üìÖ Data: 02/11/2025

================================================================================

1Ô∏è‚É£  Health Check (GET /)
--------------------------------------------------------------------------------
Status Code: 200
Resposta:
{
  "status": "ativo",
  "mensagem": "API de previs√£o B3SA3.SA operacional",
  "versao": "1.0.0",
  "modelo_carregado": true
}
‚úÖ Health check passou! Modelo est√° carregado.

[... demais testes ...]

================================================================================
                        ‚úÖ TODOS OS TESTES PASSARAM!
================================================================================

üåê API em Produ√ß√£o: https://b3sa3-api.onrender.com
üìñ Documenta√ß√£o: https://b3sa3-api.onrender.com/docs
```

### Teste 4: Documenta√ß√£o Swagger

Abra no navegador:

```
https://b3sa3-api.onrender.com/docs
```

Voc√™ ver√° a interface interativa do Swagger UI.

### Teste 5: Fazer Previs√£o

```bash
curl -X POST https://b3sa3-api.onrender.com/predict \
  -H "Content-Type: application/json" \
  -d '{
    "prices": [12.5, 12.6, 12.7, 12.8, 12.9, 13.0, 13.1, 13.2, 13.3, 13.4,
               13.5, 13.6, 13.7, 13.8, 13.9, 14.0, 14.1, 14.2, 14.3, 14.4,
               14.5, 14.6, 14.7, 14.8, 14.9, 15.0, 14.9, 14.8, 14.7, 14.6,
               14.5, 14.4, 14.3, 14.2, 14.1, 14.0, 13.9, 13.8, 13.7, 13.6,
               13.5, 13.4, 13.3, 13.2, 13.1, 13.0, 12.9, 12.8, 12.7, 12.6,
               12.5, 12.4, 12.3, 12.2, 12.1, 12.0, 11.9, 11.8, 11.7, 11.6]
  }'
```

**Resposta esperada**:
```json
{
  "preco_previsto": 11.52,
  "confianca": "alta",
  "mensagem": "Previs√£o gerada com sucesso. Modelo com MAPE de 1.53% no teste."
}
```

---

## üìà Monitoramento e Logs

### Acessar Logs em Tempo Real

1. No Dashboard do Render, selecione seu servi√ßo
2. Clique na aba **"Logs"**
3. Veja requisi√ß√µes em tempo real:

```
INFO:     127.0.0.1:57361 - "GET / HTTP/1.1" 200 OK
INFO:     127.0.0.1:52582 - "GET /info HTTP/1.1" 200 OK
INFO:     127.0.0.1:65262 - "POST /predict HTTP/1.1" 200 OK
```

### M√©tricas

Na aba **"Metrics"**, veja:
- **CPU Usage**: Uso de processador
- **Memory**: Uso de mem√≥ria
- **Bandwidth**: Tr√°fego de rede

### Events

Na aba **"Events"**, veja:
- Hist√≥rico de deploys
- Builds bem-sucedidos/falhados
- Reinicializa√ß√µes do servi√ßo

---

## ‚ö†Ô∏è Comportamento do Free Tier

### Sleep Mode

O free tier do Render tem uma caracter√≠stica importante:

- ‚è±Ô∏è **Sleep ap√≥s 15 minutos** de inatividade
- üêå **Primeira requisi√ß√£o ap√≥s sleep**: ~30 segundos
- ‚ö° **Requisi√ß√µes subsequentes**: r√°pidas (<100ms)

### Como Funciona

```
[API Ativa] ‚Üí 15 min inatividade ‚Üí [Sleep Mode]
                ‚Üì
    Primeira requisi√ß√£o (30s delay)
                ‚Üì
          [API Acordada]
                ‚Üì
    Requisi√ß√µes r√°pidas (<100ms)
```

### Isso √© Normal?

‚úÖ **SIM!** √â comportamento esperado do free tier.

### Solu√ß√µes

1. **Aceitar o delay** (recomendado para desenvolvimento/demonstra√ß√£o)
2. **Upgrade para plano pago** ($7/m√™s) - servi√ßo sempre ativo
3. **N√ÉO fazer ping peri√≥dico** - viola Terms of Service do Render

---

## üîß Troubleshooting

### Problema 1: Build Falha - Memory Error

**Sintoma**:
```
MemoryError: Unable to allocate array
```

**Causa**: Free tier tem limite de 512MB RAM

**Solu√ß√£o**:
‚úÖ J√° implementado: `tensorflow-cpu` em vez de `tensorflow`
‚úÖ Depend√™ncias otimizadas em `requirements-render.txt`

Se ainda ocorrer:
- Verificar se n√£o tem depend√™ncias extras no requirements
- Usar vers√µes exatas (sem `>=`)

### Problema 2: Modelo N√£o Encontrado

**Sintoma**:
```
FileNotFoundError: [Errno 2] No such file or directory: 'models/lstm_model_best.h5'
```

**Solu√ß√£o**:
```bash
# Verificar se modelos est√£o no Git
git ls-files | grep models/

# Se n√£o aparecer, adicionar for√ßadamente
git add -f models/lstm_model_best.h5
git add -f models/scaler.pkl
git commit -m "fix: Adicionar modelos para deploy"
git push
```

### Problema 3: Port Binding Error

**Sintoma**:
```
OSError: [Errno 98] Address already in use
```

**Causa**: Start command n√£o est√° usando `$PORT`

**Solu√ß√£o**: Sempre usar `$PORT`:
```bash
uvicorn api.main:app --host 0.0.0.0 --port $PORT
```

### Problema 4: Import Error

**Sintoma**:
```
ModuleNotFoundError: No module named 'api'
```

**Solu√ß√µes**:
```bash
# 1. Verificar se api/__init__.py existe
ls api/__init__.py

# 2. Se n√£o existir, criar
touch api/__init__.py
git add api/__init__.py
git commit -m "fix: Adicionar __init__.py"
git push
```

### Problema 5: Timeout na Primeira Requisi√ß√£o

**Sintoma**: Requisi√ß√£o demora >30 segundos

**Causa**: API estava em sleep mode (free tier)

**Solu√ß√£o**: 
- Aguardar ~30 segundos
- API "acordar√°" e ficar√° r√°pida
- Comportamento normal do free tier

---

## üìä Limita√ß√µes do Free Tier

| Recurso | Free Tier | Plano Pago |
|---------|-----------|------------|
| **RAM** | 512 MB | 2+ GB |
| **CPU** | Shared | Dedicated |
| **Bandwidth** | 100 GB/m√™s | Ilimitado |
| **Build Time** | 500 horas/m√™s | Ilimitado |
| **Sleep** | Sim (15 min) | N√£o |
| **Custom Domain** | ‚úÖ Sim | ‚úÖ Sim |
| **HTTPS** | ‚úÖ Autom√°tico | ‚úÖ Autom√°tico |
| **Custo** | **Gr√°tis** | $7+/m√™s |

---

## ‚úÖ Checklist de Conclus√£o

### Prepara√ß√£o

- [ ] `requirements-render.txt` criado
- [ ] `render.yaml` criado
- [ ] `Procfile` criado
- [ ] `.gitignore` atualizado para incluir modelos
- [ ] Modelos adicionados ao Git
- [ ] C√≥digo commitado e pushado para GitHub

### Deploy

- [ ] Conta criada no Render.com
- [ ] Reposit√≥rio conectado
- [ ] Web Service configurado
- [ ] Build conclu√≠do com sucesso
- [ ] URL p√∫blica obtida

### Testes

- [ ] Health check funcionando (GET /)
- [ ] Info do modelo respondendo (GET /info)
- [ ] M√©tricas acess√≠veis (GET /metrics)
- [ ] Previs√£o funcionando (POST /predict)
- [ ] Documenta√ß√£o Swagger acess√≠vel (/docs)
- [ ] Script `test_production.py` executado com sucesso

### Verifica√ß√£o Final

Executar:

```bash
# 1. Testar API
python test_production.py https://SUA-URL.onrender.com

# 2. Verificar documenta√ß√£o
# Abrir no navegador: https://SUA-URL.onrender.com/docs

# 3. Anotar URL para documenta√ß√£o
echo "API URL: https://SUA-URL.onrender.com" >> .env
```

**Crit√©rios de Sucesso**:
- ‚úÖ API acess√≠vel publicamente
- ‚úÖ Todos os endpoints funcionando
- ‚úÖ Previs√µes retornando valores razo√°veis
- ‚úÖ Documenta√ß√£o Swagger operacional
- ‚úÖ Logs mostrando requisi√ß√µes

---

## üìö Refer√™ncias

### Documenta√ß√£o Oficial

- **Render FastAPI Guide**: https://render.com/docs/deploy-fastapi
- **Render Free Tier**: https://render.com/docs/free
- **Render Dashboard**: https://dashboard.render.com/

### Documenta√ß√£o do Projeto

- [DEPLOY_RENDER.md](DEPLOY_RENDER.md) - Documenta√ß√£o detalhada
- [DEPLOY_QUICKSTART.md](../DEPLOY_QUICKSTART.md) - Guia r√°pido
- [test_production.py](../test_production.py) - Script de teste

### Alternativas de Deploy

Se preferir outras plataformas:
- **Railway**: https://railway.app/
- **Fly.io**: https://fly.io/
- **PythonAnywhere**: https://www.pythonanywhere.com/

Processo similar ao Render.

---

## üéØ Pr√≥ximos Passos

Ap√≥s concluir a Fase 7:

### Fase 8: Monitoramento e Finaliza√ß√£o (√∫ltima fase!)

**Objetivos**:
- Implementar logging estruturado (Loguru)
- Dashboard de monitoramento (Streamlit - opcional)
- V√≠deo explicativo (10 minutos)
- Documenta√ß√£o final completa
- README aprimorado

**Estimativa**: 2-3 horas

---

**Elaborado por**: Sistema PredictFinance  
**Data**: 02/11/2025  
**Vers√£o**: 1.0.0  
**Status**: ‚úÖ Fase 7 - Instru√ß√µes Completas
