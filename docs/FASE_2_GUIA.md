# Guia de ExecuÃ§Ã£o - Fase 2: PreparaÃ§Ã£o dos Dados para LSTM

## ğŸ“‹ Objetivo da Fase 2

Transformar os dados limpos da Fase 1 em formato adequado para treinamento da rede neural LSTM, incluindo normalizaÃ§Ã£o, criaÃ§Ã£o de sequÃªncias temporais e divisÃ£o em conjuntos de treino, validaÃ§Ã£o e teste.

---

## ğŸ”§ PrÃ©-requisitos

### 1. Fase 1 ConcluÃ­da
Certifique-se de que a Fase 1 foi executada com sucesso e o arquivo existe:
```bash
data/raw/b3sa3_historical.csv
```

### 2. DependÃªncias Instaladas
As bibliotecas necessÃ¡rias jÃ¡ devem estar instaladas do requirements.txt:
- pandas
- numpy
- scikit-learn (MinMaxScaler)
- joblib
- matplotlib
- seaborn

---

## ğŸš€ Executar Fase 2

### Comando de ExecuÃ§Ã£o
```bash
python src/data_preparation.py
```

---

## ğŸ“¤ SaÃ­das Esperadas

ApÃ³s a execuÃ§Ã£o bem-sucedida, os seguintes arquivos serÃ£o criados:

### 1. Arrays NumPy (Dados Processados)
**LocalizaÃ§Ã£o**: `data/processed/`

- **X_train.npy**: SequÃªncias de entrada para treino
  - Shape: (n_train, 60, 5)
  - n_train â‰ˆ 70% do total de sequÃªncias
  - 60 = timesteps (janela de 60 dias)
  - 5 = features (Open, High, Low, Close, Volume)

- **y_train.npy**: Valores alvo para treino
  - Shape: (n_train, 1)
  - PreÃ§os de fechamento normalizados

- **X_val.npy, y_val.npy**: ValidaÃ§Ã£o (15%)
- **X_test.npy, y_test.npy**: Teste (15%)

### 2. Scaler Persistido
**LocalizaÃ§Ã£o**: `models/scaler.pkl`

- MinMaxScaler ajustado aos dados
- NecessÃ¡rio para:
  - Desnormalizar previsÃµes futuras
  - Normalizar novos dados em produÃ§Ã£o

### 3. Log de ExecuÃ§Ã£o
**LocalizaÃ§Ã£o**: `docs/data_preparation/data_preparation_log.json`

- ParÃ¢metros utilizados
- Shapes dos conjuntos de dados
- Metadados da normalizaÃ§Ã£o

### 4. VisualizaÃ§Ãµes
**LocalizaÃ§Ã£o**: `docs/data_preparation/data_preparation_viz.png`

GrÃ¡ficos incluindo:
- ComparaÃ§Ã£o Original vs Normalizado
- DistribuiÃ§Ã£o dos dados normalizados
- DivisÃ£o temporal (treino/validaÃ§Ã£o/teste)
- Exemplo de sequÃªncia LSTM

---

## ğŸ“Š O Que o Script Faz

### 1. Carregamento de Dados
- LÃª `data/raw/b3sa3_historical.csv`
- Verifica ordem cronolÃ³gica
- Exibe primeiras linhas

### 2. NormalizaÃ§Ã£o (MinMaxScaler)
- Transforma valores para range [0, 1]
- Aplica a todas as features: Open, High, Low, Close, Volume
- **Por que normalizar?**
  - Estabiliza gradientes durante treinamento
  - Melhora convergÃªncia da LSTM
  - Evita dominÃ¢ncia de features com maior escala

### 3. CriaÃ§Ã£o de SequÃªncias Temporais
- **MÃ©todo**: Janela deslizante (sliding window)
- **Tamanho da janela**: 60 dias
- **Estrutura**:
  ```
  X[0] = dados[dia 0 a dia 59]   â†’ y[0] = Close do dia 60
  X[1] = dados[dia 1 a dia 60]   â†’ y[1] = Close do dia 61
  ...
  ```
- Cada sequÃªncia tem shape (60, 5)

### 4. DivisÃ£o Temporal (NÃ£o AleatÃ³ria!)
- **Treino**: 70% (dados mais antigos)
- **ValidaÃ§Ã£o**: 15% (dados intermediÃ¡rios)
- **Teste**: 15% (dados mais recentes)

**Importante**: A divisÃ£o Ã© temporal para evitar **data leakage**. NÃ£o podemos treinar com dados do futuro!

### 5. Salvamento
- Arrays NumPy: formato eficiente para TensorFlow
- Scaler: para uso em produÃ§Ã£o
- Logs e visualizaÃ§Ãµes: documentaÃ§Ã£o

---

## âœ… VerificaÃ§Ã£o de Sucesso

Ao final da execuÃ§Ã£o, vocÃª deve ver:

```
======================================================================
âœ… FASE 2 CONCLUÃDA COM SUCESSO!
======================================================================

ğŸ“ Arquivos gerados:
   â†’ data/processed/X_train.npy, y_train.npy
   â†’ data/processed/X_val.npy, y_val.npy
   â†’ data/processed/X_test.npy, y_test.npy
   â†’ models/scaler.pkl
   â†’ docs/data_preparation/

ğŸ“Š EstatÃ­sticas:
   â†’ SequÃªncias de treino: ~830
   â†’ SequÃªncias de validaÃ§Ã£o: ~178
   â†’ SequÃªncias de teste: ~178
   â†’ Timesteps por sequÃªncia: 60
   â†’ Features por timestep: 5

ğŸ¯ PrÃ³ximos passos:
   â†’ Execute: python src/model_training.py
   â†’ Para treinar o modelo LSTM
```

### Validar SaÃ­das

**1. Verificar arrays criados**:
```bash
# Windows
dir data\processed

# Linux/Mac
ls -lh data/processed
```

Deve mostrar 6 arquivos `.npy`.

**2. Verificar scaler**:
```bash
# Windows
dir models\scaler.pkl

# Linux/Mac
ls -lh models/scaler.pkl
```

**3. Testar carregamento (Python)**:
```python
import numpy as np
import joblib

# Carregar dados
X_train = np.load('data/processed/X_train.npy')
y_train = np.load('data/processed/y_train.npy')
scaler = joblib.load('models/scaler.pkl')

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"Scaler range: {scaler.feature_range}")
```

---

## ğŸ” Entendendo as SequÃªncias

### Exemplo PrÃ¡tico

Com 1.246 dias de dados da Fase 1 e timesteps=60:

```
Dias disponÃ­veis: 1.246
- Primeiros 60 dias: usados para criar a 1Âª sequÃªncia
- Total de sequÃªncias: 1.246 - 60 = 1.186

DivisÃ£o:
- Treino (70%): ~830 sequÃªncias (dias 0-889)
- ValidaÃ§Ã£o (15%): ~178 sequÃªncias (dias 890-1067)
- Teste (15%): ~178 sequÃªncias (dias 1068-1186)
```

### Formato dos Dados

**X_train** (entrada):
```
Shape: (830, 60, 5)
- 830 sequÃªncias
- 60 timesteps (dias)
- 5 features por dia

Exemplo X_train[0]:
[[Open_d0, High_d0, Low_d0, Close_d0, Volume_d0],
 [Open_d1, High_d1, Low_d1, Close_d1, Volume_d1],
 ...
 [Open_d59, High_d59, Low_d59, Close_d59, Volume_d59]]
```

**y_train** (alvo):
```
Shape: (830,)
- 830 valores alvo
- Cada valor = Close normalizado do dia seguinte

Exemplo:
y_train[0] = Close do dia 60 (dia apÃ³s a sequÃªncia)
```

---

## ğŸ“ˆ Por Que 60 Timesteps?

**RazÃµes tÃ©cnicas**:
1. **PadrÃ£o da indÃºstria**: Comum para sÃ©ries temporais financeiras
2. **~3 meses** de dados (~60 dias Ãºteis)
3. **BalanÃ§o**:
   - Muito curto (ex: 10 dias): contexto insuficiente
   - Muito longo (ex: 200 dias): overfitting, treino lento

**Pode ser ajustado**: Altere a constante `TIMESTEPS` no cÃ³digo se desejar experimentar.

---

## âš ï¸ PossÃ­veis Problemas e SoluÃ§Ãµes

### Problema 1: "FileNotFoundError: data/raw/b3sa3_historical.csv"
**Causa**: Fase 1 nÃ£o foi executada

**SoluÃ§Ã£o**:
```bash
python src/data_collection.py
```

### Problema 2: Shapes inconsistentes
**Causa**: Dados corrompidos ou incompletos

**SoluÃ§Ã£o**: Re-executar Fase 1 para coletar dados novamente

### Problema 3: MemÃ³ria insuficiente
**Causa**: Arrays grandes demais para RAM disponÃ­vel

**SoluÃ§Ã£o**: 
- Reduzir `TIMESTEPS`
- Usar menos features
- Aumentar RAM

---

## ğŸ“ Conceitos Importantes

### 1. MinMaxScaler vs StandardScaler

**MinMaxScaler (usado)**:
- Range: [0, 1]
- FÃ³rmula: `(x - min) / (max - min)`
- Vantagens: Valores limitados, ideal para LSTM

**StandardScaler**:
- MÃ©dia 0, desvio padrÃ£o 1
- FÃ³rmula: `(x - mean) / std`
- Valores podem ser negativos

### 2. DivisÃ£o Temporal vs AleatÃ³ria

**Temporal (usado)** âœ…:
- Respeita ordem cronolÃ³gica
- Evita data leakage
- Simula cenÃ¡rio real

**AleatÃ³ria** âŒ:
- Pode treinar com dados do futuro
- MÃ©tricas otimistas enganosas
- NÃ£o usar para sÃ©ries temporais!

### 3. Janela Deslizante

```
Dados: [d0, d1, d2, d3, d4, d5, ..., d100]
Timesteps: 3

SequÃªncias:
X[0] = [d0, d1, d2] â†’ y[0] = d3
X[1] = [d1, d2, d3] â†’ y[1] = d4
X[2] = [d2, d3, d4] â†’ y[2] = d5
...
```

---

## ğŸ“ PrÃ³ximos Passos

ApÃ³s concluir com sucesso a Fase 2, prossiga para:

**Fase 3: Treinamento do Modelo LSTM**
```bash
python src/model_training.py
```

Esta fase irÃ¡:
- Carregar dados de `data/processed/`
- Construir arquitetura LSTM
- Treinar com early stopping
- Salvar modelo em `models/`

---

**VersÃ£o**: 1.0.0  
**Ãšltima AtualizaÃ§Ã£o**: 02/11/2025  
**Autor**: ArgusPortal
