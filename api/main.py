"""
Aplica√ß√£o FastAPI para Previs√£o de Pre√ßos B3SA3.SA

Esta API serve o modelo LSTM treinado para fazer previs√µes de pre√ßos
de a√ß√µes da B3 S.A. (B3SA3.SA).

Fase 8: Inclui sistema de monitoramento de produ√ß√£o com logging estruturado.
"""

import os
import sys
import time
from pathlib import Path
from contextlib import asynccontextmanager
from typing import Dict, Any

import numpy as np
import joblib
from fastapi import FastAPI, HTTPException, status
from fastapi.responses import JSONResponse
from tensorflow.keras.models import load_model

# Adiciona o diret√≥rio raiz ao path para imports
ROOT_DIR = Path(__file__).parent.parent
sys.path.append(str(ROOT_DIR))

from api.schemas import (
    PrevisaoInput,
    PrevisaoOutput,
    HealthResponse,
    InfoModeloResponse
)

# Sistema de monitoramento (Fase 8)
from api.monitoring import get_prediction_logger, get_metrics_logger


# Vari√°veis globais para armazenar modelo e scaler
model = None
scaler = None
WINDOW_SIZE = 60
NUM_FEATURES = 5


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Gerenciador de ciclo de vida da aplica√ß√£o.
    
    Carrega o modelo e scaler na inicializa√ß√£o e libera recursos
    no encerramento.
    """
    global model, scaler
    
    # Startup: Carregar modelo e scaler
    print("üöÄ Iniciando API...")
    print("üìÇ Carregando artefatos do modelo...")
    
    try:
        # Caminhos dos artefatos
        model_path = ROOT_DIR / "models" / "lstm_model_best.h5"
        scaler_path = ROOT_DIR / "models" / "scaler.pkl"
        
        # Validar exist√™ncia dos arquivos
        if not model_path.exists():
            raise FileNotFoundError(f"Modelo n√£o encontrado: {model_path}")
        
        if not scaler_path.exists():
            raise FileNotFoundError(f"Scaler n√£o encontrado: {scaler_path}")
        
        # Carregar modelo
        print(f"   ‚îî‚îÄ Carregando modelo: {model_path}")
        model = load_model(str(model_path))
        print(f"   ‚úÖ Modelo carregado com sucesso!")
        
        # Carregar scaler
        print(f"   ‚îî‚îÄ Carregando scaler: {scaler_path}")
        scaler = joblib.load(str(scaler_path))
        print(f"   ‚úÖ Scaler carregado com sucesso!")
        
        print("‚úÖ API pronta para receber requisi√ß√µes!\n")
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar artefatos: {e}")
        raise
    
    yield
    
    # Shutdown: Limpar recursos
    print("\nüõë Encerrando API...")
    model = None
    scaler = None
    print("‚úÖ Recursos liberados.")


# Inicializar aplica√ß√£o FastAPI
app = FastAPI(
    title="API de Previs√£o B3SA3.SA",
    description="API REST para previs√£o de pre√ßos de a√ß√µes da B3 S.A. usando LSTM",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)


@app.get(
    "/",
    response_model=HealthResponse,
    summary="Health Check",
    description="Verifica se a API est√° ativa e operacional",
    tags=["Status"]
)
async def health_check() -> HealthResponse:
    """
    Endpoint de health check.
    
    Returns:
        HealthResponse: Status da API e informa√ß√µes b√°sicas
    """
    return HealthResponse(
        status="ativo",
        mensagem="API de previs√£o B3SA3.SA operacional",
        versao="1.0.0",
        modelo_carregado=(model is not None and scaler is not None)
    )


@app.get(
    "/health",
    response_model=HealthResponse,
    summary="Health Check Alternativo",
    description="Endpoint alternativo para verifica√ß√£o de sa√∫de da API",
    tags=["Status"]
)
async def health() -> HealthResponse:
    """
    Endpoint alternativo de health check.
    
    Returns:
        HealthResponse: Status da API
    """
    return await health_check()


@app.get(
    "/info",
    response_model=InfoModeloResponse,
    summary="Informa√ß√µes do Modelo",
    description="Retorna informa√ß√µes detalhadas sobre o modelo LSTM",
    tags=["Modelo"]
)
async def info_modelo() -> InfoModeloResponse:
    """
    Retorna informa√ß√µes sobre o modelo carregado.
    
    Returns:
        InfoModeloResponse: Detalhes do modelo e m√©tricas de performance
    """
    if model is None or scaler is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Modelo n√£o est√° carregado. Aguarde a inicializa√ß√£o da API."
        )
    
    return InfoModeloResponse(
        nome="LSTM_B3SA3_Predictor",
        arquitetura="LSTM - 2 camadas (64 ‚Üí 32 unidades) + Dropout (0.2)",
        parametros=30369,
        metricas={
            "RMSE": "R$ 0.26",
            "MAE": "R$ 0.20",
            "MAPE": "1.53%",
            "R2": "0.9351"
        },
        window_size=60,
        features=["Open", "High", "Low", "Close", "Volume"]
    )


@app.post(
    "/predict",
    response_model=PrevisaoOutput,
    summary="Fazer Previs√£o",
    description="Gera previs√£o de pre√ßo para o pr√≥ximo dia com base em 60 pre√ßos hist√≥ricos",
    tags=["Previs√£o"],
    status_code=status.HTTP_200_OK
)
async def fazer_previsao(previsao_input: PrevisaoInput) -> PrevisaoOutput:
    """
    Endpoint principal para fazer previs√µes.
    
    Recebe uma lista de 60 pre√ßos de fechamento consecutivos e retorna
    a previs√£o do pr√≥ximo pre√ßo.
    
    FASE 8: Inclui logging detalhado para monitoramento em produ√ß√£o.
    
    Args:
        previsao_input: Objeto contendo lista de 60 pre√ßos
        
    Returns:
        PrevisaoOutput: Previs√£o do pr√≥ximo pre√ßo
        
    Raises:
        HTTPException: Se o modelo n√£o estiver carregado ou ocorrer erro na previs√£o
    """
    # Inicializa loggers
    pred_logger = get_prediction_logger()
    metrics_logger = get_metrics_logger()
    
    # Incrementa contador de requisi√ß√µes
    metrics_logger.increment_request()
    
    # Marca in√≠cio do processamento
    start_time = time.time()
    
    # Validar se modelo e scaler est√£o carregados
    if model is None or scaler is None:
        metrics_logger.increment_error()
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Modelo n√£o est√° carregado. Aguarde a inicializa√ß√£o da API."
        )
    
    try:
        # Extrair dados da requisi√ß√£o
        dados = previsao_input.prices
        
        # Verificar n√∫mero de pre√ßos (valida√ß√£o adicional)
        if len(dados) != WINDOW_SIZE:
            metrics_logger.increment_error()
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"√â necess√°rio fornecer exatamente {WINDOW_SIZE} pre√ßos. Recebidos: {len(dados)}"
            )
        
        # Converter para numpy array e reshape para normaliza√ß√£o
        # Shape: (60,) -> (60, 1) para passar pelo scaler
        dados_array = np.array(dados).reshape(-1, 1)
        
        # Normalizar os dados usando o scaler
        dados_normalizados = scaler.transform(dados_array)
        
        # Reshape para formato esperado pelo modelo LSTM
        # Shape: (60, 1) -> (1, 60, 1) 
        # onde (batch_size, timesteps, features)
        # Nota: Como o modelo foi treinado com 5 features, vamos replicar
        # o valor normalizado para todas as 5 features
        dados_lstm = np.repeat(dados_normalizados.reshape(1, WINDOW_SIZE, 1), NUM_FEATURES, axis=2)
        
        # Fazer previs√£o
        predicao_normalizada = model.predict(dados_lstm, verbose=0)
        
        # Desnormalizar a previs√£o
        # Shape: [[valor_normalizado]] -> R$ valor_real
        predicao_real = scaler.inverse_transform(predicao_normalizada)
        
        # Extrair valor escalar
        valor_previsto = float(predicao_real[0, 0])
        
        # Calcula tempo de processamento
        processing_time = (time.time() - start_time) * 1000  # em ms
        
        # FASE 8: Log estruturado da previs√£o
        # Converte dados para formato adequado (shape 60x5 -> lista de listas)
        input_for_log = dados_lstm[0].tolist()  # Shape: (60, 5)
        
        request_id = pred_logger.log_prediction(
            input_data=input_for_log,
            prediction=valor_previsto,
            processing_time_ms=processing_time
        )
        
        # Retornar resposta
        return PrevisaoOutput(
            preco_previsto=round(valor_previsto, 2),
            confianca="alta",
            mensagem=f"Previs√£o gerada com sucesso. Modelo com MAPE de 1.53% no teste. [ID: {request_id}]"
        )
        
    except HTTPException:
        # Re-lan√ßar exce√ß√µes HTTP
        raise
        
    except Exception as e:
        # Capturar qualquer outro erro
        metrics_logger.increment_error()
        
        # Log do erro
        pred_logger.log_error(
            error_message=str(e),
            input_data=previsao_input.prices if hasattr(previsao_input, 'prices') else None
        )
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao processar previs√£o: {str(e)}"
        )


@app.get(
    "/metrics",
    summary="M√©tricas do Modelo",
    description="Retorna as m√©tricas de performance do modelo no conjunto de teste",
    tags=["Modelo"]
)
async def obter_metricas() -> Dict[str, Any]:
    """
    Retorna as m√©tricas de performance do modelo.
    
    Returns:
        Dict contendo as m√©tricas de avalia√ß√£o
    """
    if model is None or scaler is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Modelo n√£o est√° carregado."
        )
    
    return {
        "metricas_teste": {
            "RMSE": {
                "valor": "R$ 0.26",
                "descricao": "Raiz do Erro Quadr√°tico M√©dio"
            },
            "MAE": {
                "valor": "R$ 0.20",
                "descricao": "Erro Absoluto M√©dio"
            },
            "MAPE": {
                "valor": "1.53%",
                "descricao": "Erro Percentual Absoluto M√©dio",
                "interpretacao": "EXCELENTE (< 2%)"
            },
            "R2": {
                "valor": "0.9351",
                "descricao": "Coeficiente de Determina√ß√£o",
                "interpretacao": "Modelo explica 93.51% da vari√¢ncia"
            }
        },
        "parametros_modelo": {
            "window_size": 60,
            "num_features": 5,
            "camadas": "LSTM(64) + Dropout(0.2) + LSTM(32) + Dense(1)",
            "total_parametros": 30369
        },
        "dados_treinamento": {
            "periodo": "2020-11-03 a 2025-10-31",
            "total_dias": 1246,
            "sequencias_geradas": 1186,
            "divisao": {
                "treino": "70% (830 sequ√™ncias)",
                "validacao": "15% (177 sequ√™ncias)",
                "teste": "15% (179 sequ√™ncias)"
            }
        }
    }


if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("   API de Previs√£o B3SA3.SA - LSTM")
    print("=" * 60)
    print("\nüöÄ Iniciando servidor de desenvolvimento...\n")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
